{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "MINIUM = -2147483640\n",
    "PAD_TAG = \"<PAD>\"\n",
    "EMBEDDING_DIM = 400\n",
    "HIDDEN_DIM = 2048\n",
    "AMOUNT = 8000\n",
    "BATCH_SIZE = 16\n",
    "NUM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d154c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_sequence_batch(data ,word_to_ix, tag_to_ix):\n",
    "    seqs = [i[0] for i in data]\n",
    "    tags = [i[1] for i in data]\n",
    "    max_len = max([len(seq) for seq in seqs])\n",
    "    seqs_pad=[]\n",
    "    tags_pad=[]\n",
    "    for seq,tag in zip(seqs, tags):\n",
    "        seq_pad = seq + ['<PAD>'] * (max_len-len(seq))\n",
    "        tag_pad = tag + ['<PAD>'] * (max_len-len(tag))\n",
    "        seqs_pad.append(seq_pad)\n",
    "        tags_pad.append(tag_pad)\n",
    "    idxs_pad = torch.tensor([[word_to_ix[w] for w in seq] for seq in seqs_pad], dtype=torch.long).to(device)\n",
    "    tags_pad = torch.tensor([[tag_to_ix[t] for t in tag] for tag in tags_pad], dtype=torch.long).to(device)\n",
    "    return idxs_pad, tags_pad\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "#         self.hidden2tag = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "#             nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "#             nn.Linear(hidden_dim // 4, self.tagset_size)\n",
    "#         )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = MINIUM\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = MINIUM\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2 * self.num_layers, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2 * self.num_layers, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full([feats.shape[0], self.tagset_size], MINIUM).to(device)\n",
    "        init_alphas[:, self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var_list = []\n",
    "        forward_var_list.append(init_alphas)\n",
    "        for feat_index in range(feats.shape[1]):\n",
    "            gamar_r_l = torch.stack([forward_var_list[feat_index]] * feats.shape[2]).transpose(0, 1).to(device)\n",
    "            t_r1_k = torch.unsqueeze(feats[:, feat_index, :], 1).transpose(1, 2).to(device)\n",
    "            aa = gamar_r_l + t_r1_k + torch.unsqueeze(self.transitions, 0)\n",
    "            forward_var_list.append(torch.logsumexp(aa, dim=2))\n",
    "        terminal_var = forward_var_list[-1] + self.transitions[self.tag_to_ix[STOP_TAG]].repeat([feats.shape[0], 1]).to(device)\n",
    "        alpha = torch.logsumexp(terminal_var, dim=1)\n",
    "        return alpha\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).unsqueeze(dim=0).to(device)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.squeeze().to(device)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _get_lstm_features_train(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).to(device)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_feats = self.hidden2tag(lstm_out.to(device))\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(tags.shape[0]).to(device)\n",
    "        tags = torch.cat([torch.full([tags.shape[0],1],self.tag_to_ix[START_TAG], dtype=torch.long).to(device),tags],dim=1).to(device)\n",
    "        for i in range(feats.shape[1]):\n",
    "            feat=feats[:,i,:]\n",
    "            score = score + \\\n",
    "                    self.transitions[tags[:,i + 1], tags[:,i]] + feat[range(feat.shape[0]),tags[:,i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[:,-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), MINIUM).to(device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var_list = []\n",
    "        forward_var_list.append(init_vvars)\n",
    "\n",
    "        for feat_index in range(feats.shape[0]):\n",
    "            gamar_r_l = torch.stack([forward_var_list[feat_index]] * feats.shape[1])\n",
    "            gamar_r_l = torch.squeeze(gamar_r_l).to(device)\n",
    "            next_tag_var = gamar_r_l + self.transitions\n",
    "            viterbivars_t, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "            t_r1_k = torch.unsqueeze(feats[feat_index], 0).to(device)\n",
    "            forward_var_new = torch.unsqueeze(viterbivars_t, 0).to(device) + t_r1_k\n",
    "            forward_var_list.append(forward_var_new)\n",
    "            backpointers.append(bptrs_t.tolist())\n",
    "        terminal_var = forward_var_list[-1] + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var).tolist()\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood_parallel(self, sentences, tags):\n",
    "        feats = self._get_lstm_features_train(sentences).to(device)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return torch.sum(forward_score - gold_score)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence).to(device)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed47797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open('sents.train', 'r')\n",
    "test_file = open('sents.test', 'r')\n",
    "answer_file = open('sents.answer', 'r')\n",
    "\n",
    "def generate(file):\n",
    "    cur = file.readline()\n",
    "    sentences = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    while cur:\n",
    "        if cur == '\\n':\n",
    "            cur = file.readline()\n",
    "            continue\n",
    "        ws = cur.split(' ')\n",
    "        for word in ws:\n",
    "            w, label = word.split('/') if len(word.split('/')) == 2 else (''.join(word.split('/')[:-1]), word.split('/')[-1])\n",
    "            if label.endswith('\\n'): label = label[:-1]\n",
    "            if (w or label) and not (w and label):\n",
    "                print(file, word)\n",
    "                continue\n",
    "            words.append(w)\n",
    "            labels.append(label)\n",
    "        sentences.append((words, labels))\n",
    "        words = []\n",
    "        labels = []\n",
    "        cur = file.readline()\n",
    "    return sentences\n",
    "\n",
    "training_data = generate(train_file)\n",
    "testing_data = generate(answer_file)\n",
    "\n",
    "train_file.close()\n",
    "test_file.close()\n",
    "answer_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c53931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39832\n",
      "1993\n",
      "44368 48\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "word_to_ix['<PAD>'] = 0\n",
    "tag_to_ix = {}\n",
    "\n",
    "word_count = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            word_count[word] = 1\n",
    "        else: word_count[word] += 1\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "word_to_ix['ONCE_OTHER_EXCEPTION'] = len(word_to_ix)\n",
    "for i, (sentence, tags) in enumerate(training_data):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word_count[word] == 1: \n",
    "            training_data[i][0][j] = 'ONCE_OTHER_EXCEPTION'\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[PAD_TAG] = len(tag_to_ix)\n",
    "print(len(word_to_ix), len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30203dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_sequence(sentence, to_ix):\n",
    "    idxs = []\n",
    "    for w in sentence:\n",
    "        if w in to_ix: idxs.append(to_ix[w])\n",
    "        else:\n",
    "            idxs.append(to_ix['ONCE_OTHER_EXCEPTION'])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "def accuracy():\n",
    "    cor = cnt = 0\n",
    "    test_data = testing_data\n",
    "    for sentence, tags in test_data:\n",
    "        precheck_sent = prepare_test_sequence(sentence, word_to_ix).to(device)\n",
    "        answer = [tag_to_ix[t] for t in tags]\n",
    "        score, prediction = model(precheck_sent)\n",
    "        for i in range(len(answer)):\n",
    "            if answer[i] == prediction[i]: cor += 1\n",
    "        cnt += len(tags)\n",
    "    return cor, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix).to(device)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long).to(device)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(200): \n",
    "    model.zero_grad()\n",
    "    epoch_data = random.sample(training_data, AMOUNT)\n",
    "    for i in range(len(epoch_data) // BATCH_SIZE):\n",
    "        train_data = epoch_data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "        sentence_in_pad, targets_pad = prepare_sequence_batch(train_data, word_to_ix, tag_to_ix)\n",
    "        sentence_in_pad = sentence_in_pad.to(device)\n",
    "        targets_pad = targets_pad.to(device)\n",
    "        loss = model.neg_log_likelihood_parallel(sentence_in_pad, targets_pad).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    cor, cnt = accuracy()\n",
    "    print(\"epoch : \", str(epoch), \" accuracy : \", cor / cnt, \" elapse time : \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '.\\model.pkl')\n",
    "temp_model = torch.load('.\\model.pkl')\n",
    "start = time.time()\n",
    "def temp_accuracy():\n",
    "    cor = cnt = 0\n",
    "    test_data = testing_data\n",
    "    for sentence, tags in test_data:\n",
    "        precheck_sent = prepare_test_sequence(sentence, word_to_ix).to(device)\n",
    "        answer = [tag_to_ix[t] for t in tags]\n",
    "        score, prediction = temp_model(precheck_sent)\n",
    "        for i in range(len(answer)):\n",
    "            if answer[i] == prediction[i]: cor += 1\n",
    "        cnt += len(tags)\n",
    "    return cor, cnt\n",
    "cor, cnt = temp_accuracy()\n",
    "print(time.time() - start, cor / cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd8fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29f531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
