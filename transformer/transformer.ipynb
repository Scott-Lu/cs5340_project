{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3251,"status":"ok","timestamp":1650523754380,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"},"user_tz":-480},"id":"_EtYp6PxKFWZ","outputId":"83ca6853-7375-4be1-a530-41276e64c202"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yV5D2i6WKGtI"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Transformer/')"]},{"cell_type":"code","source":["!pip install pytorch-crf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AmKyG1YWLy0","executionInfo":{"status":"ok","timestamp":1650523761477,"user_tz":-480,"elapsed":7108,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"170684c4-190d-4d6a-9484-6b69331e3f83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.7/dist-packages (0.7.2)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGUmouOsYcwa"},"outputs":[],"source":["import torchtext\n","import torch\n","from torchcrf import CRF\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from collections import Counter\n","from torchtext.vocab import Vocab\n","import io\n","import re\n","from typing import Iterable, List"]},{"cell_type":"code","source":["SRC_LANGUAGE = 'word'\n","TGT_LANGUAGE = 'tag'"],"metadata":{"id":"kJ1zVc4xqYkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_transform={}\n","def word_tokenizer(line):\n","  return [wt.rsplit('/',1)[0] for wt in line.rstrip('\\n').split(' ')]\n","def tag_tokenizer(line):\n","  return [wt.rsplit('/',1)[1] for wt in line.rstrip('\\n').split(' ')]\n","token_transform[SRC_LANGUAGE]=word_tokenizer\n","token_transform[TGT_LANGUAGE]=tag_tokenizer"],"metadata":{"id":"x-554fdLx6AR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def yield_tokens(filepath,ln):\n","  language_index={'word':0,'tag':1}\n","  with io.open(filepath, encoding=\"utf8\") as f:\n","    for line in f.readlines():\n","      yield token_transform[ln](line)"],"metadata":{"id":"dPIhe8-xluUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"],"metadata":{"id":"7KcuFK5KpMGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_transform={}\n","filepath='./trainset.txt'\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(filepath, ln),\n","                            min_freq=1,\n","                            specials=special_symbols,\n","                            special_first=True)\n","\n","# Set UNK_IDX as the default index. This index is returned when the token is not found.\n","# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"],"metadata":{"id":"KGCZ2AD0qlRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                        nhead=nhead,\n","                        num_encoder_layers=num_encoder_layers,\n","                        num_decoder_layers=num_decoder_layers,\n","                        dim_feedforward=dim_feedforward,\n","                        dropout=dropout)\n","        self.crf=CRF(tgt_vocab_size)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        emission=self.generator(outs)\n","        return emission\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"metadata":{"id":"LUabV1q8rU_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"metadata":{"id":"H8RPTxemrUh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 2\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 1\n","NUM_DECODER_LAYERS = 2\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"3JSG5zBXwfwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# src and tgt language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n","                          vocab_transform[ln], #Numericalization\n","                          tensor_transform) # Add BOS/EOS and create tensor\n","\n","\n","# function to collate data samples into batch tesors\n","def collate_fn(batch):\n","    src_batch, tgt_batch, tgtinput_batch = [], [], []\n","    for sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](sample.rstrip(\"\\n\")))\n","        curtgt=text_transform[TGT_LANGUAGE](sample.rstrip(\"\\n\"))\n","        tgt_batch.append(curtgt)\n","        seqlen=curtgt.shape[0]\n","        tgtinput_batch.append(torch.tensor([BOS_IDX]*seqlen))\n","\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    tgtinput_batch = pad_sequence(tgtinput_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch,tgtinput_batch"],"metadata":{"id":"WQye1gp9w4_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAINPATH='./trainset.txt'\n","VALPATH='./valset.txt'"],"metadata":{"id":"kB65NW3S1dpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            上次验证集损失值改善后等待几个epoch\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            如果是True，为每个验证集损失值改善打印一条信息\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            监测数量的最小变化，以符合改进的要求\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''\n","        Saves model when validation loss decrease.\n","        验证损失减少时保存模型。\n","        '''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        # torch.save(model.state_dict(), 'checkpoint.pt')     # 这里会存储迄今最优模型的参数\n","        torch.save(model, 'checkpoint.pt')                 # 这里会存储迄今最优的模型\n","        self.val_loss_min = val_loss"],"metadata":{"id":"UgR7xTao-hek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    f=io.open(TRAINPATH, encoding=\"utf8\")\n","    train_iter=f.readlines()\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt, tgt_input in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","        tgt_input = tgt_input.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :] #tgt_input[:-1,:] #\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        _, tgt_mask, _, tgt_padding_mask = create_mask(src, tgt_out)\n","        tgt_padding_mask=(~tgt_padding_mask).transpose(0, 1).type(torch.uint8)\n","        loss=-model.crf(logits,tgt_out,mask=tgt_padding_mask)\n","\n","        # loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","    f=io.open(VALPATH, encoding=\"utf8\")\n","    val_iter=f.readlines()\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt, tgt_input in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","        tgt_input = tgt_input.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :] # tgt_input[:-1,:] #\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","\n","        _, tgt_mask, _, tgt_padding_mask = create_mask(src, tgt_out)\n","        tgt_padding_mask=(~tgt_padding_mask).transpose(0, 1).type(torch.uint8)\n","        loss=-model.crf(logits,tgt_out,mask=tgt_padding_mask)\n","\n","        # loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"],"metadata":{"id":"FN-IVrGPwHPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","model_parameters = filter(lambda p: p.requires_grad, transformer.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])"],"metadata":{"id":"O3HSEo1_2q5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5PTSIH83F0l","executionInfo":{"status":"ok","timestamp":1650544085296,"user_tz":-480,"elapsed":3,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"01994383-7bb6-4855-8145-efecaa40dd22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27361780"]},"metadata":{},"execution_count":193}]},{"cell_type":"code","source":["import time\n","from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","patience=3\n","early_stopping = EarlyStopping(patience=patience, verbose=True)\n","start=time.time()\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","\n","    early_stopping(val_loss, transformer)\n","    if early_stopping.early_stop:\n","      print(\"Early stopping\")\n","      break\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","end=time.time()\n","transformer=torch.load('checkpoint.pt')\n","traintime=end-start\n","traintime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhaPEnRY1u4x","executionInfo":{"status":"ok","timestamp":1650544576680,"user_tz":-480,"elapsed":490217,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"10a56d32-3064-4311-fa33-06cdb1a3daf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss decreased (inf --> 1950.334077).  Saving model ...\n","Epoch: 1, Train loss: 6128.863, Val loss: 1950.334, Epoch time = 45.896s\n","Validation loss decreased (1950.334077 --> 693.494476).  Saving model ...\n","Epoch: 2, Train loss: 1726.247, Val loss: 693.494, Epoch time = 44.411s\n","Validation loss decreased (693.494476 --> 512.690792).  Saving model ...\n","Epoch: 3, Train loss: 810.242, Val loss: 512.691, Epoch time = 44.521s\n","Validation loss decreased (512.690792 --> 463.232431).  Saving model ...\n","Epoch: 4, Train loss: 550.888, Val loss: 463.232, Epoch time = 44.179s\n","Validation loss decreased (463.232431 --> 441.909554).  Saving model ...\n","Epoch: 5, Train loss: 428.677, Val loss: 441.910, Epoch time = 44.167s\n","EarlyStopping counter: 1 out of 3\n","Epoch: 6, Train loss: 351.130, Val loss: 447.185, Epoch time = 44.100s\n","Validation loss decreased (441.909554 --> 423.246785).  Saving model ...\n","Epoch: 7, Train loss: 302.581, Val loss: 423.247, Epoch time = 43.754s\n","EarlyStopping counter: 1 out of 3\n","Epoch: 8, Train loss: 262.030, Val loss: 428.805, Epoch time = 44.155s\n","EarlyStopping counter: 2 out of 3\n","Epoch: 9, Train loss: 230.900, Val loss: 434.527, Epoch time = 43.838s\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["490.3692970275879"]},"metadata":{},"execution_count":194}]},{"cell_type":"code","source":["# function to generate output sequence using greedy algorithm\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys_ini = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    ys=ys_ini\n","    # res = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    # for i in range(max_len-1):\n","    for i in range((src.shape[0])):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        # out = out.transpose(0, 1)\n","        prob = model.generator(out)\n","        prediction = model.crf.decode(prob)\n","        ys = torch.cat([ys_ini,torch.tensor(prediction).transpose(0,1).to(DEVICE)],dim=0)\n","        # next_word=prediction.item()\n","        # _, next_word = torch.max(prob, dim=1)\n","        # next_word = next_word.item()\n","\n","        # ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        # res = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        # if next_word == EOS_IDX:\n","        #     break\n","    return ys\n","\n","\n","# actual function to translate input sentence into target language\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))))\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"],"metadata":{"id":"uUA0fEcJwt3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TESTPATH='./sents.answer'\n","f=io.open(TESTPATH, encoding=\"utf8\")\n","test_iter=f.readlines()\n","test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"],"metadata":{"id":"IJCt98BA668U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["truetag=[]\n","for line in test_iter:\n","  truetag.append(token_transform[TGT_LANGUAGE](line))\n"],"metadata":{"id":"dGTAFd-h71iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start=time.time()\n","predtag=[]\n","i=0\n","for line in test_iter:\n","  print(f'{i}/{len(test_iter)}')\n","  i+=1\n","  tmpli=translate(transformer, line).split(' ')\n","  if tmpli[0]=='':\n","    tmpli=tmpli[1:]\n","  if tmpli[-1]=='':\n","    tmpli=tmpli[:-1]\n","  predtag.append(tmpli)\n","end=time.time()\n","predtime=end-start"],"metadata":{"id":"OesJcCUA7Dxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predtime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"leqxQ-OroyDo","executionInfo":{"status":"ok","timestamp":1650544912602,"user_tz":-480,"elapsed":44,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"835fadd5-a56f-4466-d898-cf59f83c49a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["336.0183491706848"]},"metadata":{},"execution_count":196}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"metadata":{"id":"yrrdWXq398Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correctnum=0\n","allnum=0\n","for i in range(len(truetag)):\n","  try:\n","    correctnum+=accuracy_score(y_true=truetag[i],y_pred=predtag[i][:len(truetag[i])])*len(truetag[i])\n","    allnum+=len(truetag[i])\n","  except:\n","    print(i)\n","    continue"],"metadata":{"id":"HOT0POD0-RQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correctnum/allnum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBEG2a5G-7cJ","executionInfo":{"status":"ok","timestamp":1650544912955,"user_tz":-480,"elapsed":11,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"487a2764-dac1-481d-9829-dca3aa951dba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.957865345453782"]},"metadata":{},"execution_count":199}]},{"cell_type":"markdown","source":["#### Transformer+CRF\n","NHead | Encoder layer | Decoder layer | Number of Param. | Train time | Predict time | Accuracy\n","--- | --- | --- | --- | --- | --- | ---\n","1 | 1 | 1 | 24732148 | 401.999s | 254.1364s | 95.28%\n","2 | 1 | 1 | 24732148 | 405.998s | 256.352s | 95.73%\n","4 | 1 | 1 | 24732148 | 323.886s | 257.431s | 95.47%\n","1 | 2 | 1 | 26310132 | 450.551s | 254.890s | 94.93%\n","1 | 4 | 1 | 29466100 | 602.610s | 258.773s | 94.85%\n","1 | 1 | 2 | 27361780 | 530.378s | 327.641s | 95.56%\n","1 | 1 | 4 | 32621044 | 708.147s | 479.063s | 95.88%\n"],"metadata":{"id":"2qX_wK9nKtBL"}},{"cell_type":"markdown","source":["NHead | Encoder layer | Decoder layer | Number of Param. | Train time | Predict time | Accuracy\n","--- | --- | --- | --- | --- | --- | ---\n","1 | 1 | 1 | 24729649 | 281.136s | 88.057s | 95.02%\n","2 | 1 | 1 | 24729649 | 203.541s | 91.099s | 95.42%\n","4 | 1 | 1 | 24729649 | 281.731s | 88.984s | 95.51%\n","1 | 2 | 1 | 26307633 | 292.877s | 93.071s | 95.12%\n","1 | 4 | 1 | 29463601 | 430.538s | 92.004s | 94.63%\n","1 | 1 | 2 | 27359281 | 414.209s | 144.412s | 95.66%\n","1 | 1 | 4 | 32618545 | 755.313s | 251.045s | 95.81%"],"metadata":{"id":"7vubT_oUlEvc"}},{"cell_type":"code","source":[""],"metadata":{"id":"0ds5bFV0lENi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"z0cT_pz4Ksop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"psXJdg_-xOy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def argmax(vec):\n","    # return the argmax as a python int\n","    _, idx = torch.max(vec, 1)\n","    return idx\n","\n","\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","# Compute log sum exp in a numerically stable way for the forward algorithm\n","def log_sum_exp(vec):\n","    # in: [batch,tgt_size]\n","    # out: [batch]\n","    max_score,_=torch.max(vec,1)\n","    return max_score+torch.log(torch.sum(torch.exp(vec-max_score.view(vec.shape[0],-1)),1))\n","\n","    # max_score = vec[:, argmax(vec)]\n","    # max_score_broadcast = max_score.view(1, -1).expand(vec.size())\n","    # return max_score + \\\n","    #     torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n","\n","\n","class Transformer_CRF(nn.Module):\n","\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Transformer_CRF, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                        nhead=nhead,\n","                        num_encoder_layers=num_encoder_layers,\n","                        num_decoder_layers=num_decoder_layers,\n","                        dim_feedforward=dim_feedforward,\n","                        dropout=dropout)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(\n","            emb_size, dropout=dropout)\n","        \n","        self.transitions = nn.Parameter(\n","            torch.randn(tgt_vocab_size, tgt_vocab_size))\n","        \n","        self.transitions.data[BOS_IDX, :] = -10000\n","        self.transitions.data[:, EOS_IDX] = -10000\n","        self.tagset_size=tgt_vocab_size\n","\n","    def transformer_forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)\n","\n","    def _forward_alg(self, feats,tags):\n","        # feats: [seq_len,batch,hidden]\n","        # tags: [seq_len,batch]\n","        # Do the forward algorithm to compute the partition function\n","        batchsize=tags.shape[1]\n","        alpha= torch.zeros(batchsize).to(DEVICE)\n","\n","        init_alphas = torch.full((batchsize, self.tagset_size), -10000.)\n","        # START_TAG has all of the score.\n","        init_alphas[:,BOS_IDX] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","        forward_var=forward_var.to(DEVICE)\n","\n","        # Iterate through the sentence\n","        for i in range(feats.shape[0]):\n","            tag=tags[i]\n","            notpad=torch.where(tag!=PAD_IDX)[0]\n","            notpadeos=torch.where((tag!=PAD_IDX) & (tag!=EOS_IDX))[0]\n","            # if tag==EOS_IDX:\n","            #   break\n","            feat=feats[i]\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[:,next_tag].view(\n","                    batchsize, -1).expand(batchsize, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].expand(batchsize, self.tagset_size)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var\n","                next_tag_var=next_tag_var.index_add_(0, notpad, trans_score[notpad])\n","                next_tag_var=next_tag_var.index_add_(0, notpadeos, emit_score[notpadeos])\n","\n","                # next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(batchsize))\n","            forward_var = torch.cat(alphas_t).view(batchsize, -1)\n","        terminal_var = forward_var\n","        alpha = log_sum_exp(terminal_var)\n","        return torch.mean(alpha)\n","\n","    def _get_transformer_features(self, src,tgt):\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","        tgt_input = tgt[:-1, :]\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","        transformer_feats = self.transformer_forward(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","        return transformer_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # feats: [seq_len,batch,hidden]\n","        # tags: [seq_len,batch]\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(tags.shape[1]).to(DEVICE)\n","        for i, feat in enumerate(feats):\n","            notpad=torch.where(tags[i+1]!=PAD_IDX)[0]\n","            notpadeos=torch.where((tags[i+1]!=PAD_IDX) & (tags[i+1]!=EOS_IDX))[0]\n","            score=score.index_add_(0, notpad, self.transitions[tags[i + 1], tags[i]][notpad])\n","            score=score.index_add_(0, notpadeos, torch.gather(feat,1,tags[i + 1].unsqueeze(1)).squeeze(1)[notpadeos])\n","        return torch.mean(score)\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.)\n","        init_vvars[0][BOS_IDX] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[EOS_IDX]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == BOS_IDX  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_transformer_features(sentence,tags)\n","        forward_score = self._forward_alg(feats, tags)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence,tags):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_transformer_features(sentence,tags)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq"],"metadata":{"id":"xF1dYJZZ8WzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 1\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 1\n","NUM_DECODER_LAYERS = 1\n","\n","crf_transformer = Transformer_CRF(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in crf_transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","crf_transformer = crf_transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(crf_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"ojUenH1s8g4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    f=io.open(TRAINPATH, encoding=\"utf8\")\n","    train_iter=f.readlines()\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","    i=0\n","    for src, tgt, tgt_input in train_dataloader:\n","        print(f'{i}/{len(train_dataloader)}')\n","        i+=1\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","        \n","        optimizer.zero_grad()\n","\n","        loss=model.neg_log_likelihood(src,tgt[1:,:])\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","    f=io.open(VALPATH, encoding=\"utf8\")\n","    val_iter=f.readlines()\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt, tgt_input in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        loss=model.neg_log_likelihood(src,tgt)\n","        \n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"],"metadata":{"id":"jsjf-JTB9Z0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","model_parameters = filter(lambda p: p.requires_grad, crf_transformer.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])\n","params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJZJO-oc-k__","executionInfo":{"status":"ok","timestamp":1650478329718,"user_tz":-480,"elapsed":22,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"edc7e4a3-5256-4102-8d0a-e8ff1f32bf43"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24732050"]},"metadata":{},"execution_count":187}]},{"cell_type":"code","source":["import time\n","from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","patience=3\n","early_stopping = EarlyStopping(patience=patience, verbose=True)\n","start=time.time()\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(crf_transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(crf_transformer)\n","\n","    early_stopping(val_loss, crf_transformer)\n","    if early_stopping.early_stop:\n","      print(\"Early stopping\")\n","      break\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","end=time.time()\n","crf_transformer=torch.load('checkpoint.pt')\n","traintime=end-start\n","traintime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nI_fWmuM-v6M","executionInfo":{"status":"ok","timestamp":1650490293216,"user_tz":-480,"elapsed":10796170,"user":{"displayName":"Delong Zhang","userId":"14517677938115800362"}},"outputId":"3730f6df-a550-43bd-c650-db236985d940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (inf --> -180.035065).  Saving model ...\n","Epoch: 1, Train loss: -14.971, Val loss: -180.035, Epoch time = 616.917s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-180.035065 --> -369.974229).  Saving model ...\n","Epoch: 2, Train loss: -226.320, Val loss: -369.974, Epoch time = 609.616s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-369.974229 --> -573.985090).  Saving model ...\n","Epoch: 3, Train loss: -401.313, Val loss: -573.985, Epoch time = 605.626s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-573.985090 --> -780.944405).  Saving model ...\n","Epoch: 4, Train loss: -594.321, Val loss: -780.944, Epoch time = 607.969s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-780.944405 --> -1011.909864).  Saving model ...\n","Epoch: 5, Train loss: -808.934, Val loss: -1011.910, Epoch time = 603.156s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-1011.909864 --> -1300.733613).  Saving model ...\n","Epoch: 6, Train loss: -1055.607, Val loss: -1300.734, Epoch time = 603.368s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-1300.733613 --> -1580.522184).  Saving model ...\n","Epoch: 7, Train loss: -1345.697, Val loss: -1580.522, Epoch time = 604.501s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-1580.522184 --> -1858.048617).  Saving model ...\n","Epoch: 8, Train loss: -1660.760, Val loss: -1858.049, Epoch time = 643.938s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-1858.048617 --> -2151.091572).  Saving model ...\n","Epoch: 9, Train loss: -1996.980, Val loss: -2151.092, Epoch time = 628.770s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-2151.091572 --> -2439.193786).  Saving model ...\n","Epoch: 10, Train loss: -2348.254, Val loss: -2439.194, Epoch time = 617.150s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-2439.193786 --> -2711.901743).  Saving model ...\n","Epoch: 11, Train loss: -2711.834, Val loss: -2711.902, Epoch time = 636.815s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-2711.901743 --> -3081.331310).  Saving model ...\n","Epoch: 12, Train loss: -3096.433, Val loss: -3081.331, Epoch time = 621.446s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-3081.331310 --> -3333.778305).  Saving model ...\n","Epoch: 13, Train loss: -3489.996, Val loss: -3333.778, Epoch time = 629.675s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-3333.778305 --> -3639.858348).  Saving model ...\n","Epoch: 14, Train loss: -3911.118, Val loss: -3639.858, Epoch time = 632.874s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-3639.858348 --> -4024.489762).  Saving model ...\n","Epoch: 15, Train loss: -4348.270, Val loss: -4024.490, Epoch time = 621.605s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-4024.489762 --> -4413.540450).  Saving model ...\n","Epoch: 16, Train loss: -4795.854, Val loss: -4413.540, Epoch time = 627.730s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-4413.540450 --> -4843.186334).  Saving model ...\n","Epoch: 17, Train loss: -5260.998, Val loss: -4843.186, Epoch time = 618.912s\n","0/249\n","1/249\n","2/249\n","3/249\n","4/249\n","5/249\n","6/249\n","7/249\n","8/249\n","9/249\n","10/249\n","11/249\n","12/249\n","13/249\n","14/249\n","15/249\n","16/249\n","17/249\n","18/249\n","19/249\n","20/249\n","21/249\n","22/249\n","23/249\n","24/249\n","25/249\n","26/249\n","27/249\n","28/249\n","29/249\n","30/249\n","31/249\n","32/249\n","33/249\n","34/249\n","35/249\n","36/249\n","37/249\n","38/249\n","39/249\n","40/249\n","41/249\n","42/249\n","43/249\n","44/249\n","45/249\n","46/249\n","47/249\n","48/249\n","49/249\n","50/249\n","51/249\n","52/249\n","53/249\n","54/249\n","55/249\n","56/249\n","57/249\n","58/249\n","59/249\n","60/249\n","61/249\n","62/249\n","63/249\n","64/249\n","65/249\n","66/249\n","67/249\n","68/249\n","69/249\n","70/249\n","71/249\n","72/249\n","73/249\n","74/249\n","75/249\n","76/249\n","77/249\n","78/249\n","79/249\n","80/249\n","81/249\n","82/249\n","83/249\n","84/249\n","85/249\n","86/249\n","87/249\n","88/249\n","89/249\n","90/249\n","91/249\n","92/249\n","93/249\n","94/249\n","95/249\n","96/249\n","97/249\n","98/249\n","99/249\n","100/249\n","101/249\n","102/249\n","103/249\n","104/249\n","105/249\n","106/249\n","107/249\n","108/249\n","109/249\n","110/249\n","111/249\n","112/249\n","113/249\n","114/249\n","115/249\n","116/249\n","117/249\n","118/249\n","119/249\n","120/249\n","121/249\n","122/249\n","123/249\n","124/249\n","125/249\n","126/249\n","127/249\n","128/249\n","129/249\n","130/249\n","131/249\n","132/249\n","133/249\n","134/249\n","135/249\n","136/249\n","137/249\n","138/249\n","139/249\n","140/249\n","141/249\n","142/249\n","143/249\n","144/249\n","145/249\n","146/249\n","147/249\n","148/249\n","149/249\n","150/249\n","151/249\n","152/249\n","153/249\n","154/249\n","155/249\n","156/249\n","157/249\n","158/249\n","159/249\n","160/249\n","161/249\n","162/249\n","163/249\n","164/249\n","165/249\n","166/249\n","167/249\n","168/249\n","169/249\n","170/249\n","171/249\n","172/249\n","173/249\n","174/249\n","175/249\n","176/249\n","177/249\n","178/249\n","179/249\n","180/249\n","181/249\n","182/249\n","183/249\n","184/249\n","185/249\n","186/249\n","187/249\n","188/249\n","189/249\n","190/249\n","191/249\n","192/249\n","193/249\n","194/249\n","195/249\n","196/249\n","197/249\n","198/249\n","199/249\n","200/249\n","201/249\n","202/249\n","203/249\n","204/249\n","205/249\n","206/249\n","207/249\n","208/249\n","209/249\n","210/249\n","211/249\n","212/249\n","213/249\n","214/249\n","215/249\n","216/249\n","217/249\n","218/249\n","219/249\n","220/249\n","221/249\n","222/249\n","223/249\n","224/249\n","225/249\n","226/249\n","227/249\n","228/249\n","229/249\n","230/249\n","231/249\n","232/249\n","233/249\n","234/249\n","235/249\n","236/249\n","237/249\n","238/249\n","239/249\n","240/249\n","241/249\n","242/249\n","243/249\n","244/249\n","245/249\n","246/249\n","247/249\n","248/249\n","Validation loss decreased (-4843.186334 --> -5148.027677).  Saving model ...\n","Epoch: 18, Train loss: -5739.466, Val loss: -5148.028, Epoch time = 622.869s\n"]},{"output_type":"execute_result","data":{"text/plain":["11963.448099851608"]},"metadata":{},"execution_count":188}]},{"cell_type":"code","source":[""],"metadata":{"id":"HKe-772P8WVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-Quegs-fBgI"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBQ73QMmOBDreaPemEhrkV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}